{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5707d7f-1da3-4f24-a2ba-21e764d6f467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping batch S05...\n",
      "Scraping company information from https://www.ycombinator.com/companies/alacrity...\n",
      "Scraping company information from https://www.ycombinator.com/companies/parcelbio...\n",
      "Scraping company information from https://www.ycombinator.com/companies/k-scale-labs...\n",
      "An error occurred while scraping https://www.ycombinator.com/companies/k-scale-labs: Page.goto: Timeout 30000ms exceeded.\n",
      "Call log:\n",
      "navigating to \"https://www.ycombinator.com/companies/k-scale-labs\", waiting until \"load\"\n",
      "\n",
      "Saving scraped data so far and retrying...\n",
      "Company data has been saved to 'C:\\Users\\user\\company_data.json'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "# Function to load progress from a file\n",
    "def load_progress():\n",
    "    if os.path.exists(\"progress.json\"):\n",
    "        with open(\"progress.json\", \"r\") as file:\n",
    "            return json.load(file)\n",
    "    else:\n",
    "        return {\"current_batch\": None, \"links\": []}\n",
    "\n",
    "# Function to save progress to a file\n",
    "def save_progress(current_batch, links):\n",
    "    progress = {\"current_batch\": current_batch, \"links\": links}\n",
    "    with open(\"progress.json\", \"w\") as file:\n",
    "        json.dump(progress, file)\n",
    "\n",
    "# Function to save company data to a file\n",
    "def save_company_data(company_data):\n",
    "    file_path = os.path.join(os.getcwd(), \"company_data.json\")\n",
    "    with open(file_path, \"w\") as file:\n",
    "        json.dump(company_data, file, indent=4)\n",
    "    print(f\"Company data has been saved to '{file_path}'.\")\n",
    "\n",
    "# Function to scrape all batch numbers\n",
    "async def scrape_all_batch_numbers():\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch()\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(\"https://www.ycombinator.com/companies\")\n",
    "        \n",
    "        # Wait for the main content to load\n",
    "        await page.wait_for_selector('div._facet_99gj3_85')\n",
    "        \n",
    "        # Click on the \"More options\" link to load additional content\n",
    "        await page.click('a._showMoreLess_99gj3_241')\n",
    "        \n",
    "        # Wait for additional content to load\n",
    "        await page.wait_for_selector('div._facet_99gj3_85')\n",
    "        \n",
    "        # Get the HTML content after all content is loaded\n",
    "        html_content = await page.content()\n",
    "\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    batch_elements = soup.select('div._facet_99gj3_85:has(h4:-soup-contains(\"Batch\")) div:has(label)')\n",
    "\n",
    "    batch_numbers = []\n",
    "    for batch in batch_elements:\n",
    "        span = batch.find('span', class_='_label_99gj3_225')  # Find the span element\n",
    "        if span:  # Check if span element exists\n",
    "            batch_numbers.append(span.text)\n",
    "    \n",
    "    return batch_numbers\n",
    "\n",
    "async def scrape_company_links(batch_num):\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch()\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(f\"https://www.ycombinator.com/companies?batch={batch_num}\")\n",
    "        await page.wait_for_selector('a._company_99gj3_339')\n",
    "        # Scroll down the page until there are no more new companies\n",
    "        last_height = await page.evaluate(\"document.body.scrollHeight\")\n",
    "        while True:\n",
    "            try:\n",
    "                await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "                await page.wait_for_timeout(6000)  # Let the page load\n",
    "                new_height = await page.evaluate(\"document.body.scrollHeight\")\n",
    "                if new_height == last_height:\n",
    "                    break\n",
    "                last_height = new_height\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {str(e)}\")\n",
    "                print(\"Saving links scraped so far and retrying...\")\n",
    "                break\n",
    "        \n",
    "        html_content = await page.content()\n",
    "\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    company_cards = soup.find_all(\"a\", class_=\"_company_99gj3_339\")\n",
    "\n",
    "    companies_links = []\n",
    "    for card in company_cards:\n",
    "        company_url = f\"https://www.ycombinator.com{card['href']}\"\n",
    "        companies_links.append(company_url)\n",
    "\n",
    "    return companies_links\n",
    "\n",
    "# Function to scrape company information\n",
    "async def scrape_company_info(company_url):\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch()\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(company_url)\n",
    "        \n",
    "        # Wait for the main content to load\n",
    "        await page.wait_for_selector('h1.font-extralight')\n",
    "        \n",
    "        # Get the HTML content after all content is loaded\n",
    "        html_content = await page.content()\n",
    "\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    \n",
    "    # Extract company information\n",
    "    company_info = {}\n",
    "    company_info['Name'] = soup.select_one('h1.font-extralight').text.strip()\n",
    "    company_info['Tagline'] = soup.select('div.prosemax-w-full div h3').get_text(strip=True) if soup.find('div.prosemax-w-full h3') else None\n",
    "    company_info['Description'] = soup.select_one('p.whitespace-pre-line').text.strip()\n",
    "    company_info['Batch'] = soup.select_one('div.flex.flex-row.items-center.gap-\\[6px\\] span').text.strip()\n",
    "    company_info['Company Type'] = soup.select_one('div.flex.flex-row.items-center.justify-between').text.strip()\n",
    "    company_info['Industry Tags'] = [tag.text.strip() for tag in soup.select('div.align-center flex flex-row flex-wrap gap-x-2 gap-y-2 a:has(div.yc-tw-Pill.rounded-sm.bg-\\[#E6E4DC\\].uppercase.tracking-widest.px-3py-\\[3px\\].text-\\[12px\\].font-thin)')]\n",
    "    company_info['Location'] = soup.select_one('div.flex.flex-row.justify-between:contains(\"Location\")').text.strip()\n",
    "    company_info['Website URL'] = soup.select_one('a[href].mb-2.whitespace-nowrap')['href']\n",
    "    company_info['Year Founded'] = soup.select_one('div.flex.flex-row.justify-between:contains(\"Founded\")').text.strip()\n",
    "    company_info['Team Size'] = soup.select_one('div.flex.flex-row.justify-between:contains(\"Team Size\")').text.strip()\n",
    "    \n",
    "    # Extract social media links\n",
    "    social_links = soup.select('div.space-x-2 a')\n",
    "    for link in social_links:\n",
    "        if 'linkedin.com' in link['href']:\n",
    "            company_info['LinkedIn Profile'] = link['href']\n",
    "        elif 'twitter.com' in link['href']:\n",
    "            company_info['Twitter Handle'] = link['href']\n",
    "        elif 'facebook.com' in link['href']:\n",
    "            company_info['Facebook Page'] = link['href']\n",
    "    \n",
    "    # Extract founder information\n",
    "    founders = []\n",
    "    founder_elements = soup.select('div.space-y-5')\n",
    "    for founder_element in founder_elements:\n",
    "        founder = {}\n",
    "        founder['Name'] = founder_element.select_one('div.flex-grow h3.text-lg.font-bold').text.strip()\n",
    "        founder['Role'] =  founder_element.get_text().split(',')[1].strip() if (founder_element := soup.select_one('div.flex-grow h3.text-lg.font-bold')) and ',' in founder_element.get_text() else None\n",
    "        founder['Biography'] = founder_element.select_one('p.prose.max-w-full.whitespace-pre-line').text.strip() if founder_element.select_one('p.prose.max-w-full.whitespace-pre-line') else None\n",
    "        founder_links = founder_element.select('a')\n",
    "        for link in founder_links:\n",
    "            if 'linkedin.com' in link['href']:\n",
    "                founder['LinkedIn Profile'] = link['href']\n",
    "            elif 'twitter.com' in link['href']:\n",
    "                founder['Twitter Profile'] = link['href']\n",
    "        founders.append(founder)\n",
    "    \n",
    "    company_info['Founders'] = founders\n",
    "    return company_info\n",
    "\n",
    "async def main():\n",
    "    # Load progress\n",
    "    progress = load_progress()\n",
    "    current_batch = progress[\"current_batch\"]\n",
    "    links = progress[\"links\"]\n",
    "\n",
    "    if current_batch is None:\n",
    "        # Start from the beginning if no progress is saved\n",
    "        batch_numbers = await scrape_all_batch_numbers()\n",
    "        current_batch_index = 0\n",
    "    else:\n",
    "        # Continue from the last batch if progress is saved\n",
    "        batch_numbers = await scrape_all_batch_numbers()\n",
    "        current_batch_index = batch_numbers.index(current_batch)\n",
    "\n",
    "    for batch_num in batch_numbers[current_batch_index:]:\n",
    "        print(f\"Scraping batch {batch_num}...\")\n",
    "        batch_links = await scrape_company_links(batch_num)\n",
    "        links.extend(batch_links)\n",
    "        save_progress(batch_num, links)\n",
    "\n",
    "    # Scrape company information\n",
    "    company_data = []\n",
    "    for link in links:\n",
    "        print(f\"Scraping company information from {link}...\")\n",
    "        try:\n",
    "            company_info = await scrape_company_info(link)\n",
    "            company_data.append(company_info)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while scraping {link}: {str(e)}\")\n",
    "            print(\"Saving scraped data so far and retrying...\")\n",
    "            break\n",
    "    \n",
    "    # Save company data to JSON file\n",
    "    save_company_data(company_data)\n",
    "\n",
    "# Run the main function\n",
    "await main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afcffb0-191c-48d8-8fd5-89bfdfb5608c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
