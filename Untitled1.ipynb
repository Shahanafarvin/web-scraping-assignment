{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb367a50-cf93-479c-ba1e-330cc84d9f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping batch W14...\n",
      "Scraping batch S13...\n",
      "Scraping batch W13...\n",
      "Scraping batch S12...\n",
      "Scraping batch W12...\n",
      "Scraping batch S11...\n",
      "Scraping batch W11...\n",
      "Scraping batch S10...\n",
      "Scraping batch W10...\n",
      "Scraping batch S09...\n",
      "Scraping batch W09...\n",
      "Scraping batch S08...\n",
      "Scraping batch W08...\n",
      "Scraping batch S07...\n",
      "Scraping batch W07...\n",
      "Scraping batch S06...\n",
      "Scraping batch W06...\n",
      "Scraping batch S05...\n",
      "Data has been scraped and saved to 'C:\\Users\\user\\ycombinator_companies.csv'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "# Function to load progress from a file\n",
    "def load_progress():\n",
    "    if os.path.exists(\"progress.json\"):\n",
    "        with open(\"progress.json\", \"r\") as file:\n",
    "            return json.load(file)\n",
    "    else:\n",
    "        return {\"current_batch\": None, \"links\": []}\n",
    "\n",
    "# Function to save progress to a file\n",
    "def save_progress(current_batch, links):\n",
    "    progress = {\"current_batch\": current_batch, \"links\": links}\n",
    "    with open(\"progress.json\", \"w\") as file:\n",
    "        json.dump(progress, file)\n",
    "\n",
    "# Function to scrape all batch numbers\n",
    "async def scrape_all_batch_numbers():\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch()\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(\"https://www.ycombinator.com/companies\")\n",
    "        \n",
    "        # Wait for the main content to load\n",
    "        await page.wait_for_selector('div._facet_99gj3_85')\n",
    "        \n",
    "        # Click on the \"More options\" link to load additional content\n",
    "        await page.click('a._showMoreLess_99gj3_241')\n",
    "        \n",
    "        # Wait for additional content to load\n",
    "        await page.wait_for_selector('div._facet_99gj3_85')\n",
    "        \n",
    "        # Get the HTML content after all content is loaded\n",
    "        html_content = await page.content()\n",
    "\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    batch_elements = soup.select('div._facet_99gj3_85:has(h4:-soup-contains(\"Batch\")) div:has(label)')\n",
    "\n",
    "    batch_numbers = []\n",
    "    for batch in batch_elements:\n",
    "        span = batch.find('span', class_='_label_99gj3_225')  # Find the span element\n",
    "        if span:  # Check if span element exists\n",
    "            batch_numbers.append(span.text)\n",
    "    \n",
    "    return batch_numbers\n",
    "\n",
    "# Function to scrape company links\n",
    "async def scrape_company_links(batch_num):\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch()\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(f\"https://www.ycombinator.com/companies?batch={batch_num}\")\n",
    "        await page.wait_for_selector('a._company_99gj3_339')\n",
    "        # Scroll down the page until there are no more new companies\n",
    "        last_height = await page.evaluate(\"document.body.scrollHeight\")\n",
    "        while True:\n",
    "            try:\n",
    "                await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "                await page.wait_for_timeout(6000)  # Let the page load\n",
    "                new_height = await page.evaluate(\"document.body.scrollHeight\")\n",
    "                if new_height == last_height:\n",
    "                    break\n",
    "                last_height = new_height\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {str(e)}\")\n",
    "                print(\"Saving links scraped so far and retrying...\")\n",
    "                break\n",
    "        \n",
    "        html_content = await page.content()\n",
    "\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    company_cards = soup.find_all(\"a\", class_=\"_company_99gj3_339\")\n",
    "\n",
    "    companies_links = []\n",
    "    for card in company_cards:\n",
    "        company_url = f\"https://www.ycombinator.com{card['href']}\"\n",
    "        companies_links.append(company_url)\n",
    "\n",
    "    return companies_links\n",
    "\n",
    "async def main():\n",
    "    # Load progress\n",
    "    progress = load_progress()\n",
    "    current_batch = progress[\"current_batch\"]\n",
    "    links = progress[\"links\"]\n",
    "\n",
    "    if current_batch is None:\n",
    "        # Start from the beginning if no progress is saved\n",
    "        batch_numbers = await scrape_all_batch_numbers()\n",
    "        current_batch_index = 0\n",
    "    else:\n",
    "        # Continue from the last batch if progress is saved\n",
    "        batch_numbers = await scrape_all_batch_numbers()\n",
    "        current_batch_index = batch_numbers.index(current_batch)\n",
    "\n",
    "    for batch_num in batch_numbers[current_batch_index:]:\n",
    "        print(f\"Scraping batch {batch_num}...\")\n",
    "        batch_links = await scrape_company_links(batch_num)\n",
    "        links.extend(batch_links)\n",
    "        save_progress(batch_num, links)\n",
    "\n",
    "    # Convert scraped data to DataFrame\n",
    "    df = pd.DataFrame(links, columns=['links'])\n",
    "\n",
    "    # Save data to CSV file\n",
    "    file_path = os.path.join(os.getcwd(), \"ycombinator_companies.csv\")\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Data has been scraped and saved to '{file_path}'.\")\n",
    "\n",
    "# Run the main function\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9813089-697e-4dab-901f-248b9e19f5f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
